{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read AWS ID and KEY from dl.cfg configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_data = os.path.join(\"s3n://udacity-dend/\", \"song_data/A/A/A/*.json\")\n",
    "df = spark.read.json(song_data)\n",
    "    \n",
    "# df.printSchema()\n",
    "# df.show(5)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df['song_id','title','artist_id','year','duration']\n",
    "    \n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table = songs_table \\\n",
    "              .write \\\n",
    "              .partitionBy('artist_id','year') \\\n",
    "              .parquet(os.path.join('s3a://xxxx/', 'songs.pq'),'overwrite')\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df['artist_id','artist_name','artist_location','artist_latitude','artist_longitude']\n",
    "    \n",
    "# write artists table to parquet files\n",
    "artists_table = artists_table \\\n",
    "                .write \\\n",
    "                .parquet(os.path.join('s3a://xxxx/','artists.pq'),'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = os.path.join(\"s3a://udacity-dend/\", \"log_data/2018/11/*.json\")\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n",
    "    \n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = df['userId','firstName','lastName','gender','level']\n",
    "    \n",
    "# write users table to parquet files\n",
    "users_table = users_table \\\n",
    "              .write \\\n",
    "              .parquet(os.path.join('s3a:/xxxx/','users.pq'),'overwrite')\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: str(int(int(x) / 1000)))\n",
    "df = df.withColumn(\"timestamp\", get_timestamp(df.ts))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000.0)))\n",
    "df = df.withColumn(\"datetime\", get_datetime(df.ts))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select(\n",
    "    'timestamp',\n",
    "    hour('datetime').alias('hour'),\n",
    "    dayofmonth('datetime').alias('day'),\n",
    "    weekofyear('datetime').alias('week'),\n",
    "    month('datetime').alias('month'),\n",
    "    year('datetime').alias('year'),\n",
    "    date_format('datetime', 'F').alias('weekday')\n",
    ")\n",
    "    \n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table = time_table \\\n",
    "             .write \\\n",
    "             .partitionBy('year','month') \\\n",
    "             .parquet(os.path.join('s3a://xxxx/','time.pq'),'overwrite')\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(os.path.join('s3a://xxxx/', 'songs.pq'));\n",
    "df_join = df.join(song_df, song_df.title == df.song)\n",
    "# df_join.show()\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df_join['timestamp','userId','level','song_id','artist_id','sessionId','location','userAgent']\n",
    "songplays_table = songplays_table.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table = songplays_table \\\n",
    "                  .repartition(year(\"timestamp\"), month(\"timestamp\")) \\\n",
    "                  .write \\\n",
    "                  .partitionBy(\"timestamp\") \\\n",
    "                  .parquet(os.path.join('s3a://xxxx/','songplays.pq'),'overwrite')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
